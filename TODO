1) Have we discussed the API auto-retrying authentication on a 401 response?  If not, that's where I want to take the other API's, so you could be ahead of the curve if you did that.  It should be up to the user to enable re-auth or not, so maybe an argument to your Connection.new() method?  Then you'd want to update your methods that check return statuses for a 401 and re-auth.

Major and I discussed this in-person about the state-fulness of class objects versus what's in the storage system.  The bottom line is that any time you ask the storage system for information, it could be outdated, right?  It kinda rubbed Major the wrong way when I suggested he capture these values and set them as instance variables since they could get outdated.  However, it's a fallacy to think that re-fetching them each time the user calls "count" or "bytes" is "accurate".  It just reduces the time that the data may by "stale", right?  If I have a script that's uploading files continuously, each time I call the Connection's "bytes" method, that value will be outdated on the next upload.

You could extend the same argument for lists of containers/objects and that those lists should be kept in instance variables too.  The reason why we don't is that that can be memory consuming in an "infinite" storage system.  If I can have an unlimited number of containers/ objects, that could be bad for your health.

Another question comes to mind while reviewing it.  Do you (or the underlying httplib) URL encode/decode container/object/metadata?  That obviously will help with passing in character names with non-alpha like []<>()=+-$. and utf8.

In Container.object, you're calling the container instance's populate method.  Per my previous email, I wouldn't bother with that.  You're initiating two HEAD requests (storage+cdn) to try and keep the container instance's attributes updated because you've added a new storage object.  If the user wants an updated view, they can call
connection.get_container() again.

Container.create_object allows the user to set a 'noclobber' option if I'm reading that correctly.  The other API's do not support that. Their 'create_object' calls do not talk to the storage system at all. The 'get_object' calls will throw a 404=>NoSuchObject if the remote storage object does not exist.  Looks like your storage_object.initialize works the same way.

Ok, the big one I see that needs fixing is storage_object.data() (and the commented out data_stream()) method.  You're probably already working on this, but the user needs a way to pass a ruby 'block' to the method that can read from the stream in chunks.  Take a look at the AWS gem; they've got some great examples in there.

The same applies to object.write().  The data argument should support either an in-memory string, block, or byte-stream.

TESTS: Great work, looks like a comprehensive set of test coverage!   
Can I ask that you also throw in some tests for UTF8 container/object names too?
